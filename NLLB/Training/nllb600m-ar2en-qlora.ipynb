{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T18:54:53.128966Z",
     "iopub.status.busy": "2024-08-03T18:54:53.128064Z",
     "iopub.status.idle": "2024-08-03T18:56:55.889778Z",
     "shell.execute_reply": "2024-08-03T18:56:55.888672Z",
     "shell.execute_reply.started": "2024-08-03T18:54:53.128930Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.43.3\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.32.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.5.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m73.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.2\n",
      "Collecting peft\n",
      "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.42.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.32.1)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.5.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "Successfully installed peft-0.12.0\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=43d0a2945819b9a53b845458ef2b43eb35016fa8943daf1ff3218d8266b5ce5e\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m865.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.2.2)\n",
      "Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: portalocker, sacrebleu\n",
      "Successfully installed portalocker-2.10.1 sacrebleu-2.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install datasets \n",
    "!pip install evaluate       \n",
    "!pip install peft\n",
    "!pip install rouge_score\n",
    "!pip3 install sentencepiece -q\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:56:55.893241Z",
     "iopub.status.busy": "2024-08-03T18:56:55.891897Z",
     "iopub.status.idle": "2024-08-03T18:56:56.659396Z",
     "shell.execute_reply": "2024-08-03T18:56:56.658498Z",
     "shell.execute_reply.started": "2024-08-03T18:56:55.893197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "login(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "os.environ[\"WANDB_PROJECT\"] = \"NLLB\"\n",
    "os.environ[\"WANDB_NOTES\"] = \"Fine tune nllb\"\n",
    "os.environ[\"WANDB_NAME\"] = \"finetune-NLLB-600M-on-opus100-Ar2En-with-Qlora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:56:56.661215Z",
     "iopub.status.busy": "2024-08-03T18:56:56.660917Z",
     "iopub.status.idle": "2024-08-03T18:57:01.745801Z",
     "shell.execute_reply": "2024-08-03T18:57:01.744879Z",
     "shell.execute_reply.started": "2024-08-03T18:56:56.661189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1950029b7f21498ebc04389769657d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/65.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['af-en', 'am-en', 'an-en', 'ar-de', 'ar-en', 'ar-fr', 'ar-nl', 'ar-ru', 'ar-zh', 'as-en', 'az-en', 'be-en', 'bg-en', 'bn-en', 'br-en', 'bs-en', 'ca-en', 'cs-en', 'cy-en', 'da-en', 'de-en', 'de-fr', 'de-nl', 'de-ru', 'de-zh', 'dz-en', 'el-en', 'en-eo', 'en-es', 'en-et', 'en-eu', 'en-fa', 'en-fi', 'en-fr', 'en-fy', 'en-ga', 'en-gd', 'en-gl', 'en-gu', 'en-ha', 'en-he', 'en-hi', 'en-hr', 'en-hu', 'en-hy', 'en-id', 'en-ig', 'en-is', 'en-it', 'en-ja', 'en-ka', 'en-kk', 'en-km', 'en-kn', 'en-ko', 'en-ku', 'en-ky', 'en-li', 'en-lt', 'en-lv', 'en-mg', 'en-mk', 'en-ml', 'en-mn', 'en-mr', 'en-ms', 'en-mt', 'en-my', 'en-nb', 'en-ne', 'en-nl', 'en-nn', 'en-no', 'en-oc', 'en-or', 'en-pa', 'en-pl', 'en-ps', 'en-pt', 'en-ro', 'en-ru', 'en-rw', 'en-se', 'en-sh', 'en-si', 'en-sk', 'en-sl', 'en-sq', 'en-sr', 'en-sv', 'en-ta', 'en-te', 'en-tg', 'en-th', 'en-tk', 'en-tr', 'en-tt', 'en-ug', 'en-uk', 'en-ur', 'en-uz', 'en-vi', 'en-wa', 'en-xh', 'en-yi', 'en-yo', 'en-zh', 'en-zu', 'fr-nl', 'fr-ru', 'fr-zh', 'nl-ru', 'nl-zh', 'ru-zh']\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "configs=get_dataset_config_names(\"opus100\")\n",
    "print(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:57:01.749382Z",
     "iopub.status.busy": "2024-08-03T18:57:01.748571Z",
     "iopub.status.idle": "2024-08-03T18:57:11.823162Z",
     "shell.execute_reply": "2024-08-03T18:57:11.822269Z",
     "shell.execute_reply.started": "2024-08-03T18:57:01.749345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7055142db9e4b969abaee7d5617bfb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/214k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf20248974da487a84dd68b038d6e30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/99.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c29df5ba39a40019a942148f301bcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/979k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828fc2f7ef4b4a54816d6ee207ca6a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aade79789ea240aaa68f9b233bdb2094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1715c2f160b47958282be8aca94fa7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset=load_dataset(\"opus100\", \"ar-en\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:57:11.824626Z",
     "iopub.status.busy": "2024-08-03T18:57:11.824333Z",
     "iopub.status.idle": "2024-08-03T18:57:17.559117Z",
     "shell.execute_reply": "2024-08-03T18:57:17.558024Z",
     "shell.execute_reply.started": "2024-08-03T18:57:11.824601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c01a8c72d245f2b218cff4efb4cfab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5edab3d3217450ab98d5d11eae8c473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e2c3ef004648618d0063f50783ccbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5992a34b36b04deab68c32631ded7333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import NllbTokenizerFast\n",
    "import torch\n",
    "model_name=\"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "tokenizer = NllbTokenizerFast.from_pretrained(\n",
    "        \"facebook/nllb-200-distilled-600M\", src_lang=\"Arabic\", tgt_lang=\"English\",  device_map={'':torch.cuda.current_device()}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:57:17.560885Z",
     "iopub.status.busy": "2024-08-03T18:57:17.560425Z",
     "iopub.status.idle": "2024-08-03T18:57:21.198421Z",
     "shell.execute_reply": "2024-08-03T18:57:21.197410Z",
     "shell.execute_reply.started": "2024-08-03T18:57:17.560861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6ff4e2bf3148448c364d9895c9f811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb095bd19907495e90ba1ed3e345104f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset=dataset['train'].shuffle(seed=42).select(range(8000))\n",
    "\n",
    "# as evaluation dataset\n",
    "eval_dataset=dataset['validation'].shuffle(seed=42).select(range(1600))\n",
    "\n",
    "test_dataset=dataset['test'].shuffle(seed=42).select(range(1600))\n",
    "\n",
    "def preprocess_func(data):\n",
    "    inputs=[ex['ar'] for ex in data['translation']]\n",
    "    targets=[ex['en'] for ex in data['translation']]\n",
    "    \n",
    "    # tokenize each row of inputs and outputs\n",
    "    model_inputs=tokenizer(inputs, truncation=True)\n",
    "    labels=tokenizer(targets, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"]=labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# We tokenize the entire dataset  \n",
    "\n",
    "train_dataset=train_dataset.map(preprocess_func, batched=True)\n",
    "eval_dataset=eval_dataset.map(preprocess_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:57:21.208905Z",
     "iopub.status.busy": "2024-08-03T18:57:21.208195Z",
     "iopub.status.idle": "2024-08-03T18:57:38.806500Z",
     "shell.execute_reply": "2024-08-03T18:57:38.805297Z",
     "shell.execute_reply.started": "2024-08-03T18:57:21.208879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3f2fe53c0c4d788e61c74b1df428e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210555434df1482ba7956e635ee62c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7464f264a01b4fb09e38c99413acdf76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, prepare_model_for_kbit_training, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\", #The non-uniform\n",
    "    #nature of NF4 allows it to compress the weights into 4 bits while retaining more accuracy compared to traditional uniform 4-bit quantization\n",
    "   bnb_4bit_use_double_quant=True, # Enables double quantization to compress the quantized values further\n",
    "   bnb_4bit_compute_dtype=torch. # Specifies that the model computations should be done in bfloat16\n",
    ")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, quantization_config=nf4_config, device_map={'':torch.cuda.current_device()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:57:38.808431Z",
     "iopub.status.busy": "2024-08-03T18:57:38.808118Z",
     "iopub.status.idle": "2024-08-03T18:57:38.945204Z",
     "shell.execute_reply": "2024-08-03T18:57:38.944228Z",
     "shell.execute_reply.started": "2024-08-03T18:57:38.808404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 616,253,440 || trainable%: 0.1914\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "        target_modules=['q_proj','v_proj'], task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    "    )\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:57:38.949948Z",
     "iopub.status.busy": "2024-08-03T18:57:38.949658Z",
     "iopub.status.idle": "2024-08-03T18:57:44.477517Z",
     "shell.execute_reply": "2024-08-03T18:57:44.475874Z",
     "shell.execute_reply.started": "2024-08-03T18:57:38.949925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M2M100Config {\n",
      "  \"_name_or_path\": \"facebook/nllb-200-distilled-600M\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"scale_embedding\": true,\n",
      "  \"tokenizer_class\": \"NllbTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256206\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:57:44.479970Z",
     "iopub.status.busy": "2024-08-03T18:57:44.479409Z",
     "iopub.status.idle": "2024-08-03T18:57:44.601172Z",
     "shell.execute_reply": "2024-08-03T18:57:44.600008Z",
     "shell.execute_reply.started": "2024-08-03T18:57:44.479941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): M2M100ForConditionalGeneration(\n",
       "      (model): M2M100Model(\n",
       "        (shared): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
       "        (encoder): M2M100Encoder(\n",
       "          (embed_tokens): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
       "          (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x M2M100EncoderLayer(\n",
       "              (self_attn): M2M100Attention(\n",
       "                (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): ReLU()\n",
       "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): M2M100Decoder(\n",
       "          (embed_tokens): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
       "          (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x M2M100DecoderLayer(\n",
       "              (self_attn): M2M100Attention(\n",
       "                (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): M2M100Attention(\n",
       "                (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=256206, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:57:44.604524Z",
     "iopub.status.busy": "2024-08-03T18:57:44.603964Z",
     "iopub.status.idle": "2024-08-03T18:57:58.648381Z",
     "shell.execute_reply": "2024-08-03T18:57:58.647138Z",
     "shell.execute_reply.started": "2024-08-03T18:57:44.604486Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-03 18:57:47.282462: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-03 18:57:47.282578: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-03 18:57:47.403488: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47bdbee6781549e3b28b21f99a6a9275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dea569b7649438faae12508e4baa3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import sacrebleu\n",
    "\n",
    "metric_bleu = evaluate.load(\"sacrebleu\")\n",
    "metric_rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Define a function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "# Define a function to compute metrics\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    result_bleu = metric_bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result_bleu = {\"bleu\": result_bleu[\"score\"]}\n",
    " \n",
    "    \n",
    "    # Compute ROUGE score\n",
    "    result_rouge = metric_rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result_rouge = {\"rouge\": result_rouge[\"rougeL\"]}\n",
    "    \n",
    "    # Combine BLEU and ROUGE results\n",
    "    result = {**result_bleu, **result_rouge}\n",
    "    \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    \n",
    "    # Save the result to a file after each evaluation\n",
    "    with open('metrics.txt', 'a') as f:\n",
    "        f.write(f\"BLEU: {result['bleu']}, ROUGE: {result['rouge']}, Gen Len: {result['gen_len']}\\n\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T18:57:58.650819Z",
     "iopub.status.busy": "2024-08-03T18:57:58.650008Z",
     "iopub.status.idle": "2024-08-03T18:57:58.669961Z",
     "shell.execute_reply": "2024-08-03T18:57:58.669086Z",
     "shell.execute_reply.started": "2024-08-03T18:57:58.650784Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataCollatorForSeq2Seq(tokenizer=NllbTokenizerFast(name_or_path='facebook/nllb-200-distilled-600M', vocab_size=256204, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['ace_Arab', 'ace_Latn', 'acm_Arab', 'acq_Arab', 'aeb_Arab', 'afr_Latn', 'ajp_Arab', 'aka_Latn', 'amh_Ethi', 'apc_Arab', 'arb_Arab', 'ars_Arab', 'ary_Arab', 'arz_Arab', 'asm_Beng', 'ast_Latn', 'awa_Deva', 'ayr_Latn', 'azb_Arab', 'azj_Latn', 'bak_Cyrl', 'bam_Latn', 'ban_Latn', 'bel_Cyrl', 'bem_Latn', 'ben_Beng', 'bho_Deva', 'bjn_Arab', 'bjn_Latn', 'bod_Tibt', 'bos_Latn', 'bug_Latn', 'bul_Cyrl', 'cat_Latn', 'ceb_Latn', 'ces_Latn', 'cjk_Latn', 'ckb_Arab', 'crh_Latn', 'cym_Latn', 'dan_Latn', 'deu_Latn', 'dik_Latn', 'dyu_Latn', 'dzo_Tibt', 'ell_Grek', 'eng_Latn', 'epo_Latn', 'est_Latn', 'eus_Latn', 'ewe_Latn', 'fao_Latn', 'pes_Arab', 'fij_Latn', 'fin_Latn', 'fon_Latn', 'fra_Latn', 'fur_Latn', 'fuv_Latn', 'gla_Latn', 'gle_Latn', 'glg_Latn', 'grn_Latn', 'guj_Gujr', 'hat_Latn', 'hau_Latn', 'heb_Hebr', 'hin_Deva', 'hne_Deva', 'hrv_Latn', 'hun_Latn', 'hye_Armn', 'ibo_Latn', 'ilo_Latn', 'ind_Latn', 'isl_Latn', 'ita_Latn', 'jav_Latn', 'jpn_Jpan', 'kab_Latn', 'kac_Latn', 'kam_Latn', 'kan_Knda', 'kas_Arab', 'kas_Deva', 'kat_Geor', 'knc_Arab', 'knc_Latn', 'kaz_Cyrl', 'kbp_Latn', 'kea_Latn', 'khm_Khmr', 'kik_Latn', 'kin_Latn', 'kir_Cyrl', 'kmb_Latn', 'kon_Latn', 'kor_Hang', 'kmr_Latn', 'lao_Laoo', 'lvs_Latn', 'lij_Latn', 'lim_Latn', 'lin_Latn', 'lit_Latn', 'lmo_Latn', 'ltg_Latn', 'ltz_Latn', 'lua_Latn', 'lug_Latn', 'luo_Latn', 'lus_Latn', 'mag_Deva', 'mai_Deva', 'mal_Mlym', 'mar_Deva', 'min_Latn', 'mkd_Cyrl', 'plt_Latn', 'mlt_Latn', 'mni_Beng', 'khk_Cyrl', 'mos_Latn', 'mri_Latn', 'zsm_Latn', 'mya_Mymr', 'nld_Latn', 'nno_Latn', 'nob_Latn', 'npi_Deva', 'nso_Latn', 'nus_Latn', 'nya_Latn', 'oci_Latn', 'gaz_Latn', 'ory_Orya', 'pag_Latn', 'pan_Guru', 'pap_Latn', 'pol_Latn', 'por_Latn', 'prs_Arab', 'pbt_Arab', 'quy_Latn', 'ron_Latn', 'run_Latn', 'rus_Cyrl', 'sag_Latn', 'san_Deva', 'sat_Beng', 'scn_Latn', 'shn_Mymr', 'sin_Sinh', 'slk_Latn', 'slv_Latn', 'smo_Latn', 'sna_Latn', 'snd_Arab', 'som_Latn', 'sot_Latn', 'spa_Latn', 'als_Latn', 'srd_Latn', 'srp_Cyrl', 'ssw_Latn', 'sun_Latn', 'swe_Latn', 'swh_Latn', 'szl_Latn', 'tam_Taml', 'tat_Cyrl', 'tel_Telu', 'tgk_Cyrl', 'tgl_Latn', 'tha_Thai', 'tir_Ethi', 'taq_Latn', 'taq_Tfng', 'tpi_Latn', 'tsn_Latn', 'tso_Latn', 'tuk_Latn', 'tum_Latn', 'tur_Latn', 'twi_Latn', 'tzm_Tfng', 'uig_Arab', 'ukr_Cyrl', 'umb_Latn', 'urd_Arab', 'uzn_Latn', 'vec_Latn', 'vie_Latn', 'war_Latn', 'wol_Latn', 'xho_Latn', 'ydd_Hebr', 'yor_Latn', 'yue_Hant', 'zho_Hans', 'zho_Hant', 'zul_Latn']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256001: AddedToken(\"ace_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256002: AddedToken(\"ace_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256003: AddedToken(\"acm_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256004: AddedToken(\"acq_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256005: AddedToken(\"aeb_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256006: AddedToken(\"afr_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256007: AddedToken(\"ajp_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256008: AddedToken(\"aka_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256009: AddedToken(\"amh_Ethi\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256010: AddedToken(\"apc_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256011: AddedToken(\"arb_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256012: AddedToken(\"ars_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256013: AddedToken(\"ary_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256014: AddedToken(\"arz_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256015: AddedToken(\"asm_Beng\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256016: AddedToken(\"ast_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256017: AddedToken(\"awa_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256018: AddedToken(\"ayr_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256019: AddedToken(\"azb_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256020: AddedToken(\"azj_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256021: AddedToken(\"bak_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256022: AddedToken(\"bam_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256023: AddedToken(\"ban_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256024: AddedToken(\"bel_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256025: AddedToken(\"bem_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256026: AddedToken(\"ben_Beng\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256027: AddedToken(\"bho_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256028: AddedToken(\"bjn_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256029: AddedToken(\"bjn_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256030: AddedToken(\"bod_Tibt\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256031: AddedToken(\"bos_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256032: AddedToken(\"bug_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256033: AddedToken(\"bul_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256034: AddedToken(\"cat_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256035: AddedToken(\"ceb_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256036: AddedToken(\"ces_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256037: AddedToken(\"cjk_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256038: AddedToken(\"ckb_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256039: AddedToken(\"crh_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256040: AddedToken(\"cym_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256041: AddedToken(\"dan_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256042: AddedToken(\"deu_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256043: AddedToken(\"dik_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256044: AddedToken(\"dyu_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256045: AddedToken(\"dzo_Tibt\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256046: AddedToken(\"ell_Grek\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256047: AddedToken(\"eng_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256048: AddedToken(\"epo_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256049: AddedToken(\"est_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256050: AddedToken(\"eus_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256051: AddedToken(\"ewe_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256052: AddedToken(\"fao_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256053: AddedToken(\"pes_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256054: AddedToken(\"fij_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256055: AddedToken(\"fin_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256056: AddedToken(\"fon_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256057: AddedToken(\"fra_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256058: AddedToken(\"fur_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256059: AddedToken(\"fuv_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256060: AddedToken(\"gla_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256061: AddedToken(\"gle_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256062: AddedToken(\"glg_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256063: AddedToken(\"grn_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256064: AddedToken(\"guj_Gujr\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256065: AddedToken(\"hat_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256066: AddedToken(\"hau_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256067: AddedToken(\"heb_Hebr\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256068: AddedToken(\"hin_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256069: AddedToken(\"hne_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256070: AddedToken(\"hrv_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256071: AddedToken(\"hun_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256072: AddedToken(\"hye_Armn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256073: AddedToken(\"ibo_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256074: AddedToken(\"ilo_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256075: AddedToken(\"ind_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256076: AddedToken(\"isl_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256077: AddedToken(\"ita_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256078: AddedToken(\"jav_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256079: AddedToken(\"jpn_Jpan\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256080: AddedToken(\"kab_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256081: AddedToken(\"kac_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256082: AddedToken(\"kam_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256083: AddedToken(\"kan_Knda\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256084: AddedToken(\"kas_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256085: AddedToken(\"kas_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256086: AddedToken(\"kat_Geor\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256087: AddedToken(\"knc_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256088: AddedToken(\"knc_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256089: AddedToken(\"kaz_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256090: AddedToken(\"kbp_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256091: AddedToken(\"kea_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256092: AddedToken(\"khm_Khmr\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256093: AddedToken(\"kik_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256094: AddedToken(\"kin_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256095: AddedToken(\"kir_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256096: AddedToken(\"kmb_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256097: AddedToken(\"kon_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256098: AddedToken(\"kor_Hang\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256099: AddedToken(\"kmr_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256100: AddedToken(\"lao_Laoo\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256101: AddedToken(\"lvs_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256102: AddedToken(\"lij_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256103: AddedToken(\"lim_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256104: AddedToken(\"lin_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256105: AddedToken(\"lit_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256106: AddedToken(\"lmo_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256107: AddedToken(\"ltg_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256108: AddedToken(\"ltz_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256109: AddedToken(\"lua_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256110: AddedToken(\"lug_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256111: AddedToken(\"luo_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256112: AddedToken(\"lus_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256113: AddedToken(\"mag_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256114: AddedToken(\"mai_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256115: AddedToken(\"mal_Mlym\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256116: AddedToken(\"mar_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256117: AddedToken(\"min_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256118: AddedToken(\"mkd_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256119: AddedToken(\"plt_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256120: AddedToken(\"mlt_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256121: AddedToken(\"mni_Beng\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256122: AddedToken(\"khk_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256123: AddedToken(\"mos_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256124: AddedToken(\"mri_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256125: AddedToken(\"zsm_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256126: AddedToken(\"mya_Mymr\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256127: AddedToken(\"nld_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256128: AddedToken(\"nno_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256129: AddedToken(\"nob_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256130: AddedToken(\"npi_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256131: AddedToken(\"nso_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256132: AddedToken(\"nus_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256133: AddedToken(\"nya_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256134: AddedToken(\"oci_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256135: AddedToken(\"gaz_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256136: AddedToken(\"ory_Orya\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256137: AddedToken(\"pag_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256138: AddedToken(\"pan_Guru\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256139: AddedToken(\"pap_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256140: AddedToken(\"pol_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256141: AddedToken(\"por_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256142: AddedToken(\"prs_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256143: AddedToken(\"pbt_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256144: AddedToken(\"quy_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256145: AddedToken(\"ron_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256146: AddedToken(\"run_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256147: AddedToken(\"rus_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256148: AddedToken(\"sag_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256149: AddedToken(\"san_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256150: AddedToken(\"sat_Beng\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256151: AddedToken(\"scn_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256152: AddedToken(\"shn_Mymr\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256153: AddedToken(\"sin_Sinh\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256154: AddedToken(\"slk_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256155: AddedToken(\"slv_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256156: AddedToken(\"smo_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256157: AddedToken(\"sna_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256158: AddedToken(\"snd_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256159: AddedToken(\"som_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256160: AddedToken(\"sot_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256161: AddedToken(\"spa_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256162: AddedToken(\"als_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256163: AddedToken(\"srd_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256164: AddedToken(\"srp_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256165: AddedToken(\"ssw_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256166: AddedToken(\"sun_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256167: AddedToken(\"swe_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256168: AddedToken(\"swh_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256169: AddedToken(\"szl_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256170: AddedToken(\"tam_Taml\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256171: AddedToken(\"tat_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256172: AddedToken(\"tel_Telu\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256173: AddedToken(\"tgk_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256174: AddedToken(\"tgl_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256175: AddedToken(\"tha_Thai\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256176: AddedToken(\"tir_Ethi\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256177: AddedToken(\"taq_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256178: AddedToken(\"taq_Tfng\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256179: AddedToken(\"tpi_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256180: AddedToken(\"tsn_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256181: AddedToken(\"tso_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256182: AddedToken(\"tuk_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256183: AddedToken(\"tum_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256184: AddedToken(\"tur_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256185: AddedToken(\"twi_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256186: AddedToken(\"tzm_Tfng\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256187: AddedToken(\"uig_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256188: AddedToken(\"ukr_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256189: AddedToken(\"umb_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256190: AddedToken(\"urd_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256191: AddedToken(\"uzn_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256192: AddedToken(\"vec_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256193: AddedToken(\"vie_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256194: AddedToken(\"war_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256195: AddedToken(\"wol_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256196: AddedToken(\"xho_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256197: AddedToken(\"ydd_Hebr\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256198: AddedToken(\"yor_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256199: AddedToken(\"yue_Hant\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256200: AddedToken(\"zho_Hans\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256201: AddedToken(\"zho_Hant\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256202: AddedToken(\"zul_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256203: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
      "}, model=PeftModelForSeq2SeqLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): M2M100ForConditionalGeneration(\n",
      "      (model): M2M100Model(\n",
      "        (shared): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
      "        (encoder): M2M100Encoder(\n",
      "          (embed_tokens): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
      "          (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
      "          (layers): ModuleList(\n",
      "            (0-11): 12 x M2M100EncoderLayer(\n",
      "              (self_attn): M2M100Attention(\n",
      "                (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (activation_fn): ReLU()\n",
      "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): M2M100Decoder(\n",
      "          (embed_tokens): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
      "          (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
      "          (layers): ModuleList(\n",
      "            (0-11): 12 x M2M100DecoderLayer(\n",
      "              (self_attn): M2M100Attention(\n",
      "                (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (encoder_attn): M2M100Attention(\n",
      "                (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1024, out_features=256206, bias=False)\n",
      "    )\n",
      "  )\n",
      "), padding=True, max_length=None, pad_to_multiple_of=8, label_pad_token_id=-100, return_tensors='pt')\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# ignore tokenizer pad token in the loss\n",
    "label_pad_token_id=-100\n",
    "\n",
    "# padding the sentence of the entire datasets\n",
    "data_collator=DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "print(data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:57:58.671442Z",
     "iopub.status.busy": "2024-08-03T18:57:58.671101Z",
     "iopub.status.idle": "2024-08-03T18:57:58.699306Z",
     "shell.execute_reply": "2024-08-03T18:57:58.698190Z",
     "shell.execute_reply.started": "2024-08-03T18:57:58.671417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:57:58.701119Z",
     "iopub.status.busy": "2024-08-03T18:57:58.700790Z",
     "iopub.status.idle": "2024-08-03T18:57:59.738759Z",
     "shell.execute_reply": "2024-08-03T18:57:59.737612Z",
     "shell.execute_reply.started": "2024-08-03T18:57:58.701093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:57:59.741350Z",
     "iopub.status.busy": "2024-08-03T18:57:59.740616Z",
     "iopub.status.idle": "2024-08-03T18:57:59.746296Z",
     "shell.execute_reply": "2024-08-03T18:57:59.745403Z",
     "shell.execute_reply.started": "2024-08-03T18:57:59.741308Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T18:57:59.747423Z",
     "iopub.status.busy": "2024-08-03T18:57:59.747165Z",
     "iopub.status.idle": "2024-08-03T22:03:51.089715Z",
     "shell.execute_reply": "2024-08-03T22:03:51.088450Z",
     "shell.execute_reply.started": "2024-08-03T18:57:59.747401Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmoudjaramina2001\u001b[0m (\u001b[33mFinalProject_\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20240803_185803-li2er79u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/FinalProject_/NLLB/runs/li2er79u' target=\"_blank\">NLLB_QLoRA</a></strong> to <a href='https://wandb.ai/FinalProject_/NLLB' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/FinalProject_/NLLB' target=\"_blank\">https://wandb.ai/FinalProject_/NLLB</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/FinalProject_/NLLB/runs/li2er79u' target=\"_blank\">https://wandb.ai/FinalProject_/NLLB/runs/li2er79u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2625/2625 3:05:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.858000</td>\n",
       "      <td>1.402344</td>\n",
       "      <td>30.549300</td>\n",
       "      <td>0.577100</td>\n",
       "      <td>17.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.464900</td>\n",
       "      <td>1.344727</td>\n",
       "      <td>31.343000</td>\n",
       "      <td>0.588600</td>\n",
       "      <td>17.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.424700</td>\n",
       "      <td>1.333984</td>\n",
       "      <td>31.594500</td>\n",
       "      <td>0.590600</td>\n",
       "      <td>17.338000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2625, training_loss=1.7456440080915179, metrics={'train_runtime': 11149.1724, 'train_samples_per_second': 1.884, 'train_steps_per_second': 0.235, 'total_flos': 1445536627949568.0, 'train_loss': 1.7456440080915179, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainerCallback, Seq2SeqTrainer, Seq2SeqTrainingArguments, Trainer, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import gc\n",
    "import torch.nn.init as init\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.eval_losses = []\n",
    "        self.steps = []\n",
    "\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        if 'loss' in state.log_history[-1]:\n",
    "            self.train_losses.append(state.log_history[-1]['loss'])\n",
    "            self.steps.append(state.global_step)\n",
    "        if 'eval_loss' in state.log_history[-1]:\n",
    "            self.eval_losses.append(state.log_history[-1]['eval_loss'])\n",
    "            \n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"NLLB_QLoRA\",\n",
    "        logging_dir=os.getenv(\"WANDB_NAME\")+\"/logs\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=500,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        weight_decay=0.01,\n",
    "        fp16=False,\n",
    "        num_train_epochs=3,\n",
    "        predict_with_generate=True,\n",
    "        load_best_model_at_end=True,\n",
    "        gradient_accumulation_steps=4,\n",
    "        save_total_limit=2,    \n",
    ")\n",
    "loss_logger = LossLoggerCallback()\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[loss_logger],\n",
    ")\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T22:03:51.101967Z",
     "iopub.status.busy": "2024-08-03T22:03:51.101605Z",
     "iopub.status.idle": "2024-08-03T22:32:07.174197Z",
     "shell.execute_reply": "2024-08-03T22:32:07.173320Z",
     "shell.execute_reply.started": "2024-08-03T22:03:51.101937Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 28:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 3.80\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results=trainer.evaluate() \n",
    "print(eval_results)\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T22:32:08.727336Z",
     "iopub.status.busy": "2024-08-03T22:32:08.727060Z",
     "iopub.status.idle": "2024-08-03T22:32:12.825564Z",
     "shell.execute_reply": "2024-08-03T22:32:12.824453Z",
     "shell.execute_reply.started": "2024-08-03T22:32:08.727312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47f8d8201a44d0e9a992bc56f5885cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d471bd53a5421c93abddef26ae6a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa91c12a32c64c40bb4f4b30b5833cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/4.74M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7729f093297045cdb362281863cd13c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/yasmineee/NLLB_QLoRA/commit/dec0cc5253198f7af1486635fbefec00cf7f9382', commit_message='finetune-NLLB-600M-on-opus100-Ar2En-with-Qlora', commit_description='', oid='dec0cc5253198f7af1486635fbefec00cf7f9382', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))\n",
    "trainer.push_to_hub(os.getenv(\"WANDB_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T22:32:07.175910Z",
     "iopub.status.busy": "2024-08-03T22:32:07.175614Z",
     "iopub.status.idle": "2024-08-03T22:32:08.725450Z",
     "shell.execute_reply": "2024-08-03T22:32:08.719606Z",
     "shell.execute_reply.started": "2024-08-03T22:32:07.175884Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1797: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Horse, night, and the plow knows me, sword, spear, plow, and pen.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# text=\" المملكة العربية السعودية\"\n",
    "# translator=pipeline(\"translation\",model=os.getenv(\"WANDB_NAME\"))\n",
    "# translator(text)\n",
    "model.config.use_cache=True\n",
    "context=tokenizer([\"ألخَيْـلُ وَاللّيْـلُ وَالبَيْـداءُ تَعرِفُنـي وَالسّيفُ وَالرّمحُ والقرْطاسُ وَالقَلَـمُ\"], return_tensors=\"pt\")\n",
    "output=model.generate(**context)\n",
    "\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
