{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets sacrebleu \n!pip install evaluate       \n!pip install peft\n!pip install rouge_score\n!pip install bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:50:20.229934Z","iopub.execute_input":"2024-08-03T12:50:20.230343Z","iopub.status.idle":"2024-08-03T12:51:34.587755Z","shell.execute_reply.started":"2024-08-03T12:50:20.230296Z","shell.execute_reply":"2024-08-03T12:51:34.586159Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nRequirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (2.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2.10.1)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.2.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.2)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.20.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.12.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.42.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.32.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: rouge_score in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"NLLB_2\"\nos.environ[\"WANDB_NOTES\"] = \"Fine tune NLLB\"\nos.environ[\"WANDB_NAME\"] = \"finetune-NLLB-600M-on-opus100-Ar2En-with-lora\"","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:51:34.591044Z","iopub.execute_input":"2024-08-03T12:51:34.591579Z","iopub.status.idle":"2024-08-03T12:51:34.968913Z","shell.execute_reply.started":"2024-08-03T12:51:34.591526Z","shell.execute_reply":"2024-08-03T12:51:34.967768Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import get_dataset_config_names\n\nconfigs=get_dataset_config_names(\"opus100\")\nprint(configs)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:51:34.970691Z","iopub.execute_input":"2024-08-03T12:51:34.971159Z","iopub.status.idle":"2024-08-03T12:51:38.266220Z","shell.execute_reply.started":"2024-08-03T12:51:34.971114Z","shell.execute_reply":"2024-08-03T12:51:38.265188Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"['af-en', 'am-en', 'an-en', 'ar-de', 'ar-en', 'ar-fr', 'ar-nl', 'ar-ru', 'ar-zh', 'as-en', 'az-en', 'be-en', 'bg-en', 'bn-en', 'br-en', 'bs-en', 'ca-en', 'cs-en', 'cy-en', 'da-en', 'de-en', 'de-fr', 'de-nl', 'de-ru', 'de-zh', 'dz-en', 'el-en', 'en-eo', 'en-es', 'en-et', 'en-eu', 'en-fa', 'en-fi', 'en-fr', 'en-fy', 'en-ga', 'en-gd', 'en-gl', 'en-gu', 'en-ha', 'en-he', 'en-hi', 'en-hr', 'en-hu', 'en-hy', 'en-id', 'en-ig', 'en-is', 'en-it', 'en-ja', 'en-ka', 'en-kk', 'en-km', 'en-kn', 'en-ko', 'en-ku', 'en-ky', 'en-li', 'en-lt', 'en-lv', 'en-mg', 'en-mk', 'en-ml', 'en-mn', 'en-mr', 'en-ms', 'en-mt', 'en-my', 'en-nb', 'en-ne', 'en-nl', 'en-nn', 'en-no', 'en-oc', 'en-or', 'en-pa', 'en-pl', 'en-ps', 'en-pt', 'en-ro', 'en-ru', 'en-rw', 'en-se', 'en-sh', 'en-si', 'en-sk', 'en-sl', 'en-sq', 'en-sr', 'en-sv', 'en-ta', 'en-te', 'en-tg', 'en-th', 'en-tk', 'en-tr', 'en-tt', 'en-ug', 'en-uk', 'en-ur', 'en-uz', 'en-vi', 'en-wa', 'en-xh', 'en-yi', 'en-yo', 'en-zh', 'en-zu', 'fr-nl', 'fr-ru', 'fr-zh', 'nl-ru', 'nl-zh', 'ru-zh']\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset=load_dataset(\"opus100\", \"ar-en\")\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:51:38.268780Z","iopub.execute_input":"2024-08-03T12:51:38.269144Z","iopub.status.idle":"2024-08-03T12:51:42.677663Z","shell.execute_reply.started":"2024-08-03T12:51:38.269105Z","shell.execute_reply":"2024-08-03T12:51:42.676402Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    test: Dataset({\n        features: ['translation'],\n        num_rows: 2000\n    })\n    train: Dataset({\n        features: ['translation'],\n        num_rows: 1000000\n    })\n    validation: Dataset({\n        features: ['translation'],\n        num_rows: 2000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import NllbTokenizerFast\nimport torch\nmodel_name=\"facebook/nllb-200-distilled-600M\"\n\ntokenizer = NllbTokenizerFast.from_pretrained(\n        \"facebook/nllb-200-distilled-600M\", src_lang=\"Arabic\", tgt_lang=\"English\", load_in_8bit=True, device_map={'':torch.cuda.current_device()}\n    )\n# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:51:42.679326Z","iopub.execute_input":"2024-08-03T12:51:42.679785Z","iopub.status.idle":"2024-08-03T12:51:44.382775Z","shell.execute_reply.started":"2024-08-03T12:51:42.679745Z","shell.execute_reply":"2024-08-03T12:51:44.381506Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# use a sample of around 2000 instead of the complete dataset as training dataset\ntrain_dataset=dataset['train'].shuffle(seed=42).select(range(7000))\n\n# as evaluation dataset\neval_dataset=dataset['validation']\n\n\ndef preprocess_func(data):\n    inputs=[ex['ar'] for ex in data['translation']]\n    targets=[ex['en'] for ex in data['translation']]\n    \n    # tokenize each row of inputs and outputs\n    model_inputs=tokenizer(inputs, truncation=True)\n    labels=tokenizer(targets, truncation=True)\n    \n    model_inputs[\"labels\"]=labels[\"input_ids\"]\n    return model_inputs\n\n\n# We tokenize the entire dataset  \n\ntrain_dataset=train_dataset.map(preprocess_func, batched=True)\neval_dataset=eval_dataset.map(preprocess_func, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:51:44.384211Z","iopub.execute_input":"2024-08-03T12:51:44.384678Z","iopub.status.idle":"2024-08-03T12:51:47.659465Z","shell.execute_reply.started":"2024-08-03T12:51:44.384636Z","shell.execute_reply":"2024-08-03T12:51:47.657775Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de284a55671243bd85793f2c6c50e1c8"}},"metadata":{}}]},{"cell_type":"code","source":"from peft import PeftModel, prepare_model_for_kbit_training, PeftConfig, get_peft_model, LoraConfig, TaskType\nfrom transformers import BitsAndBytesConfig\nfrom transformers import AutoModelForSeq2SeqLM\n\nbnb_config=BitsAndBytesConfig(\n    load_in_8bit=True\n)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name, quantization_config=bnb_config, device_map={'':torch.cuda.current_device()})","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:51:47.661313Z","iopub.execute_input":"2024-08-03T12:51:47.661795Z","iopub.status.idle":"2024-08-03T12:51:51.856617Z","shell.execute_reply.started":"2024-08-03T12:51:47.661743Z","shell.execute_reply":"2024-08-03T12:51:51.855387Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"peft_config = LoraConfig(\n        target_modules=['q_proj','v_proj'], task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n    )\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:51:51.858254Z","iopub.execute_input":"2024-08-03T12:51:51.858706Z","iopub.status.idle":"2024-08-03T12:51:52.019294Z","shell.execute_reply.started":"2024-08-03T12:51:51.858668Z","shell.execute_reply":"2024-08-03T12:51:52.018268Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"trainable params: 1,179,648 || all params: 616,253,440 || trainable%: 0.1914\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model.config)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:51:52.020576Z","iopub.execute_input":"2024-08-03T12:51:52.020874Z","iopub.status.idle":"2024-08-03T12:51:52.028910Z","shell.execute_reply.started":"2024-08-03T12:51:52.020848Z","shell.execute_reply":"2024-08-03T12:51:52.027773Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"M2M100Config {\n  \"_name_or_path\": \"facebook/nllb-200-distilled-600M\",\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"relu\",\n  \"architectures\": [\n    \"M2M100ForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"max_length\": 200,\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"m2m_100\",\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": false,\n    \"_load_in_8bit\": true,\n    \"bnb_4bit_compute_dtype\": \"float32\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"fp4\",\n    \"bnb_4bit_use_double_quant\": false,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": false,\n    \"load_in_8bit\": true,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"scale_embedding\": true,\n  \"tokenizer_class\": \"NllbTokenizer\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.42.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 256206\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:51:52.032725Z","iopub.execute_input":"2024-08-03T12:51:52.033063Z","iopub.status.idle":"2024-08-03T12:51:52.050508Z","shell.execute_reply.started":"2024-08-03T12:51:52.033035Z","shell.execute_reply":"2024-08-03T12:51:52.049395Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"PeftModelForSeq2SeqLM(\n  (base_model): LoraModel(\n    (model): M2M100ForConditionalGeneration(\n      (model): M2M100Model(\n        (shared): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n        (encoder): M2M100Encoder(\n          (embed_tokens): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n          (embed_positions): M2M100SinusoidalPositionalEmbedding()\n          (layers): ModuleList(\n            (0-11): 12 x M2M100EncoderLayer(\n              (self_attn): M2M100Attention(\n                (k_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n                (v_proj): lora.Linear8bitLt(\n                  (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_proj): lora.Linear8bitLt(\n                  (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n              )\n              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (activation_fn): ReLU()\n              (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (decoder): M2M100Decoder(\n          (embed_tokens): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n          (embed_positions): M2M100SinusoidalPositionalEmbedding()\n          (layers): ModuleList(\n            (0-11): 12 x M2M100DecoderLayer(\n              (self_attn): M2M100Attention(\n                (k_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n                (v_proj): lora.Linear8bitLt(\n                  (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_proj): lora.Linear8bitLt(\n                  (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n              )\n              (activation_fn): ReLU()\n              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (encoder_attn): M2M100Attention(\n                (k_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n                (v_proj): lora.Linear8bitLt(\n                  (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_proj): lora.Linear8bitLt(\n                  (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n              )\n              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (lm_head): Linear(in_features=1024, out_features=256206, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"import evaluate\nimport numpy as np\nimport sacrebleu\n\nmetric_bleu = evaluate.load(\"sacrebleu\")\nmetric_rouge = evaluate.load(\"rouge\")\n\n# Define a function to postprocess text\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\n# Define a function to compute metrics\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n    \n    # Compute BLEU score\n    result_bleu = metric_bleu.compute(predictions=decoded_preds, references=decoded_labels)\n    result_bleu = {\"bleu\": result_bleu[\"score\"]}\n#     references=decoded_labels\n#     result_bleu = sacrebleu.corpus_bleu(translations=decoded_preds,[references])  \n#     result_bleu = {\"bleu\": round(bleu.score, 2)}\n    \n    # Compute ROUGE score\n    result_rouge = metric_rouge.compute(predictions=decoded_preds, references=decoded_labels)\n    result_rouge = {\"rouge\": result_rouge[\"rougeL\"]}\n    \n    # Combine BLEU and ROUGE results\n    result = {**result_bleu, **result_rouge}\n    \n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    \n    # Save the result to a file after each evaluation\n    with open('metrics.txt', 'a') as f:\n        f.write(f\"BLEU: {result['bleu']}, ROUGE: {result['rouge']}, Gen Len: {result['gen_len']}\\n\")\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:51:52.052105Z","iopub.execute_input":"2024-08-03T12:51:52.052508Z","iopub.status.idle":"2024-08-03T12:51:52.631626Z","shell.execute_reply.started":"2024-08-03T12:51:52.052478Z","shell.execute_reply":"2024-08-03T12:51:52.630409Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:51:52.632965Z","iopub.execute_input":"2024-08-03T12:51:52.633287Z","iopub.status.idle":"2024-08-03T12:51:52.639623Z","shell.execute_reply.started":"2024-08-03T12:51:52.633259Z","shell.execute_reply":"2024-08-03T12:51:52.638391Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # Should print True if CUDA is available","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:51:52.641140Z","iopub.execute_input":"2024-08-03T12:51:52.641521Z","iopub.status.idle":"2024-08-03T12:51:52.651969Z","shell.execute_reply.started":"2024-08-03T12:51:52.641490Z","shell.execute_reply":"2024-08-03T12:51:52.650800Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}]},{"cell_type":"code","source":"!export CUDA_LAUNCH_BLOCKING=1","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:51:52.653427Z","iopub.execute_input":"2024-08-03T12:51:52.653811Z","iopub.status.idle":"2024-08-03T12:51:53.874865Z","shell.execute_reply.started":"2024-08-03T12:51:52.653775Z","shell.execute_reply":"2024-08-03T12:51:53.873436Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:51:53.876927Z","iopub.execute_input":"2024-08-03T12:51:53.877432Z","iopub.status.idle":"2024-08-03T12:51:53.885011Z","shell.execute_reply.started":"2024-08-03T12:51:53.877387Z","shell.execute_reply":"2024-08-03T12:51:53.883587Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainerCallback, Seq2SeqTrainer, Seq2SeqTrainingArguments, Trainer, get_linear_schedule_with_warmup\nimport torch\nimport gc\nimport torch.nn.init as init\nimport os\nimport matplotlib.pyplot as plt\nclass LossLoggerCallback(TrainerCallback):\n    def __init__(self):\n        self.train_losses = []\n        self.eval_losses = []\n        self.steps = []\n\n    def on_log(self, args, state, control, **kwargs):\n        if 'loss' in state.log_history[-1]:\n            self.train_losses.append(state.log_history[-1]['loss'])\n            self.steps.append(state.global_step)\n        if 'eval_loss' in state.log_history[-1]:\n            self.eval_losses.append(state.log_history[-1]['eval_loss'])\n            \n\ntraining_args = Seq2SeqTrainingArguments(\n        output_dir=\"NLLB_LoRA\",\n        logging_dir=os.getenv(\"WANDB_NAME\")+\"/logs\",\n        evaluation_strategy=\"epoch\",\n        learning_rate=2e-5,\n        save_strategy=\"epoch\",\n        logging_steps=500,\n        per_device_train_batch_size=1,\n        per_device_eval_batch_size=1,\n        weight_decay=0.01,\n        fp16=False,\n        num_train_epochs=3,\n        predict_with_generate=True,\n        load_best_model_at_end=True,\n        gradient_accumulation_steps=4,\n        save_total_limit=2,    \n)\nloss_logger = LossLoggerCallback()\n\ntrainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n        callbacks=[loss_logger],\n)\n\n\ngc.collect()\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:51:53.886905Z","iopub.execute_input":"2024-08-03T12:51:53.887942Z","iopub.status.idle":"2024-08-03T16:34:47.268629Z","shell.execute_reply.started":"2024-08-03T12:51:53.887896Z","shell.execute_reply":"2024-08-03T16:34:47.267517Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2625' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2625/2625 3:42:50, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Rouge</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.670200</td>\n      <td>1.393555</td>\n      <td>31.662600</td>\n      <td>0.584800</td>\n      <td>17.512500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.457200</td>\n      <td>1.337891</td>\n      <td>32.428200</td>\n      <td>0.592000</td>\n      <td>17.353000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.426200</td>\n      <td>1.328125</td>\n      <td>32.676300</td>\n      <td>0.593200</td>\n      <td>17.377000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2625, training_loss=1.6945704403831845, metrics={'train_runtime': 13372.2075, 'train_samples_per_second': 1.57, 'train_steps_per_second': 0.196, 'total_flos': 1289917550100480.0, 'train_loss': 1.6945704403831845, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"import math\n\neval_results=trainer.evaluate() \nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-03T16:34:47.270041Z","iopub.execute_input":"2024-08-03T16:34:47.270422Z","iopub.status.idle":"2024-08-03T17:22:36.553900Z","shell.execute_reply.started":"2024-08-03T16:34:47.270391Z","shell.execute_reply":"2024-08-03T17:22:36.552852Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 47:37]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Perplexity: 3.77\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.model.save_pretrained(save_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:22:36.555450Z","iopub.execute_input":"2024-08-03T17:22:36.555787Z","iopub.status.idle":"2024-08-03T17:22:36.629094Z","shell.execute_reply.started":"2024-08-03T17:22:36.555758Z","shell.execute_reply":"2024-08-03T17:22:36.627397Z"},"trusted":true},"execution_count":34,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[43msave_path\u001b[49m)\n","\u001b[0;31mNameError\u001b[0m: name 'save_path' is not defined"],"ename":"NameError","evalue":"name 'save_path' is not defined","output_type":"error"}]},{"cell_type":"code","source":"#### peft_model.config.use_cache=True\ncontext=tokenizer([\"Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©\"], return_tensors=\"pt\")\noutput=model.generate(**context)\n\ntokenizer.decode(output[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:22:36.630168Z","iopub.status.idle":"2024-08-03T17:22:36.631458Z","shell.execute_reply.started":"2024-08-03T17:22:36.631107Z","shell.execute_reply":"2024-08-03T17:22:36.631135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))\ntrainer.push_to_hub(os.getenv(\"WANDB_NAME\"))","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:22:36.632633Z","iopub.status.idle":"2024-08-03T17:22:36.633038Z","shell.execute_reply.started":"2024-08-03T17:22:36.632846Z","shell.execute_reply":"2024-08-03T17:22:36.632862Z"},"trusted":true},"execution_count":null,"outputs":[]}]}